# N-Gram Language Model with Smoothing (Streamlit App)

This project implements a Bigram (2-gram) Language Model using Natural Language Processing. It supports two smoothing techniquesâ€”**Laplace Smoothing** and **Good-Turing Smoothing**â€”to estimate probabilities for seen and unseen word pairs. The model is interactive and runs in a Streamlit web app.

ğŸš€ Features

- ğŸ“ Upload your own `.txt` corpus
- ğŸ§  Generate bigrams and count their frequencies
- ğŸ“Š Apply Laplace and Good-Turing smoothing
- ğŸ” Enter custom bigrams to check their probabilities
- ğŸ“ˆ Compare probabilities side-by-side

---

## ğŸ› ï¸ Requirements

Install the required Python packages:
pip install -r requirements.txt


## â–¶ï¸ How to Run

streamlit run app.py

## ğŸ“‚ File Structure

ngram_language_model/
â”‚
â”œâ”€â”€ app.py                # Main Streamlit application
â”œâ”€â”€ ngram_utils.py        # Utility functions for preprocessing and probability calculation
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ long_sample_corpus.txt# Example text corpus


## ğŸ“š Smoothing Techniques
Laplace Smoothing: Adds 1 to every bigram count to avoid zero probabilities.
Good-Turing Smoothing: Recalculates probability based on the frequency of frequency counts.

## ğŸ§ª Example Test Bigrams
("language", "models")
("speech", "recognition")
("deep", "learning") (Unseen bigram)

## ğŸ§  Educational Goals
This project is great for:
Understanding the mechanics of N-gram models
Seeing the effects of smoothing
Hands-on exploration of NLP probability models

